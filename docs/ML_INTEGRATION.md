# Machine Learning Integration

Comprehensive machine learning integration for Gress stream processing, enabling real-time ML inference, online learning, and model serving at scale.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Quick Start](#quick-start)
- [Model Formats](#model-formats)
- [Operators](#operators)
- [Configuration](#configuration)
- [Examples](#examples)
- [Performance](#performance)
- [Monitoring](#monitoring)

## Overview

The ML integration brings production-grade machine learning capabilities to Gress, allowing you to:

- **Real-time Inference**: Run ML models on streaming data with microsecond latency
- **Model Versioning**: Manage multiple model versions with A/B testing support
- **Online Learning**: Update models incrementally from streaming data
- **Feature Engineering**: Extract and transform features from events
- **Auto-scaling**: Dynamically scale inference based on load
- **Multi-format Support**: ONNX, TensorFlow Lite, and more

## Features

### âœ… Core Features

- **TensorFlow Lite Integration**: Lightweight inference with TFLite models
- **ONNX Runtime Support**: Industry-standard model format with optimized runtime
- **Model Registry**: Centralized model versioning and lifecycle management
- **Batch Inference**: Automatic batching for throughput optimization
- **Feature Extraction**: 10+ built-in feature transformations
- **A/B Testing**: Traffic splitting with shadow mode for model comparison
- **Online Learning**: Incremental model updates from stream
- **GPU Acceleration**: Optional GPU support for compute-intensive models
- **Checkpointing**: Model state persistence for fault tolerance
- **Prometheus Metrics**: Comprehensive observability

### ðŸŽ¯ Use Cases

- **Fraud Detection**: Real-time transaction scoring
- **Recommendations**: Personalized content and product recommendations
- **Anomaly Detection**: Identifying unusual patterns in streams
- **Predictive Maintenance**: Equipment failure prediction
- **Sentiment Analysis**: Real-time text classification
- **Time Series Forecasting**: Stream-based predictions

## Architecture

### Component Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Stream Processing Engine                 â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Kafka    â”‚â”€â”€â–¶â”‚   Feature    â”‚â”€â”€â–¶â”‚     ML       â”‚      â”‚
â”‚  â”‚   Source   â”‚   â”‚  Extraction  â”‚   â”‚  Inference   â”‚â”€â”€â”   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚                                                          â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  Model     â”‚â—€â”€â”€â”‚   Online     â”‚â—€â”€â”€â”‚   A/B Test   â”‚â—€â”€â”˜   â”‚
â”‚  â”‚  Registry  â”‚   â”‚  Learning    â”‚   â”‚   Operator   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Prometheus â”‚â—€â”€â”€â”‚    Metrics   â”‚â—€â”€â”€â”‚  Checkpoint  â”‚      â”‚
â”‚  â”‚  Metrics   â”‚   â”‚  Collector   â”‚   â”‚   Manager    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
Event Stream â†’ Feature Extraction â†’ ML Inference â†’ Post-processing â†’ Output
                     â†“                    â†“              â†“
                State Backend      Model Registry   Metrics
```

## Quick Start

### 1. Configure ML Integration

Add ML configuration to your `config.yaml`:

```yaml
ml:
  enabled: true

  model_registry:
    type: local
    base_path: ./models

  models:
    - name: fraud-detector
      version: v2.0
      type: onnx
      path: ./models/fraud-detector-v2.onnx
      enabled: true
      warmup_on_load: true

  inference:
    default_batch_size: 100
    default_batch_timeout: 50ms
    max_concurrency: 10
    max_latency: 100ms
```

### 2. Load Models

```go
import (
    "github.com/therealutkarshpriyadarshi/gress/pkg/ml"
)

// Create model registry
registry := ml.NewModelRegistry()

// Load ONNX model
metadata := ml.ModelMetadata{
    Name:    "fraud-detector",
    Version: "v2.0",
}

model, err := ml.LoadModel(ctx, ml.ModelTypeONNX,
    "./models/fraud-detector-v2.onnx", metadata)

registry.Register(model)
```

### 3. Create ML Pipeline

```go
// Feature extraction
featureConfig := ml.FeatureExtractionConfig{
    Features: []ml.FeatureSpec{
        {
            Name:   "amount_normalized",
            Type:   ml.FeatureTypeNormalize,
            Source: "amount",
            Parameters: map[string]interface{}{
                "min": 0.0,
                "max": 10000.0,
            },
        },
    },
}

featureOp, _ := ml.NewFeatureExtractionOperator(featureConfig, stateBackend)

// ML inference with batching
inferenceConfig := ml.InferenceConfig{
    ModelName:    "fraud-detector",
    ModelVersion: "v2.0",
    BatchSize:    100,
    BatchTimeout: 50 * time.Millisecond,
}

inferenceOp, _ := ml.NewInferenceOperator(inferenceConfig, registry, metrics)

// Add to stream pipeline
engine.AddOperator(featureOp)
engine.AddOperator(inferenceOp)
```

## Model Formats

### ONNX Runtime

**Advantages:**
- Industry standard format
- Framework agnostic (PyTorch, TensorFlow, Scikit-learn)
- Optimized runtime
- Wide operator support

**Usage:**
```go
model, err := ml.NewONNXModel(ctx, "model.onnx", metadata)
```

**Export from PyTorch:**
```python
import torch.onnx

# Export model to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=13
)
```

### TensorFlow Lite

**Advantages:**
- Lightweight and fast
- Optimized for edge devices
- Quantization support
- Small model size

**Usage:**
```go
model, err := ml.NewTensorFlowModel(ctx, "model.tflite", metadata)
```

**Convert from TensorFlow:**
```python
import tensorflow as tf

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_saved_model("saved_model/")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save model
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

## Operators

### Feature Extraction Operator

Transform raw events into ML features.

**Supported Transformations:**

| Type | Description | Example |
|------|-------------|---------|
| `numerical` | Extract numeric values | `amount: 500.0` |
| `categorical` | Extract categories | `category: "electronics"` |
| `onehot` | One-hot encoding | `{A: 1, B: 0, C: 0}` |
| `normalize` | Min-max normalization | `(x - min) / (max - min)` |
| `standardize` | Z-score normalization | `(x - mean) / stddev` |
| `log` | Log transformation | `log(x)` |
| `bucket` | Discretize into bins | `[0, 1, 2, 3]` |
| `temporal` | Time-based features | `{hour: 14, day: 1}` |
| `text` | Text features | `{length, word_count}` |
| `aggregation` | Window aggregations | `sum, avg, min, max` |

**Example:**
```go
features := []ml.FeatureSpec{
    {
        Name:   "amount_log",
        Type:   ml.FeatureTypeLog,
        Source: "amount",
        Output: "amount_log",
    },
    {
        Name:   "time_features",
        Type:   ml.FeatureTypeTemporal,
        Output: "time_features",
    },
}
```

### ML Inference Operator

Run ML predictions on events with automatic batching.

**Features:**
- Automatic batching for throughput
- Configurable batch size and timeout
- Concurrency control
- Latency tracking

**Configuration:**
```go
config := ml.InferenceConfig{
    ModelName:      "my-model",
    ModelVersion:   "v1.0",
    BatchSize:      100,      // Max batch size
    BatchTimeout:   10ms,     // Max wait time
    MaxConcurrency: 10,       // Parallel requests
    FeatureFields:  []string{"f1", "f2"},
    OutputField:    "prediction",
}
```

### A/B Testing Operator

Compare multiple model versions with traffic splitting.

**Strategies:**
- `random`: Random selection
- `hash`: Consistent hashing on split key
- `percentage`: Percentage-based split
- `user_id`: User-based assignment

**Shadow Mode:**
- Runs all variants in parallel
- Returns only primary variant's prediction
- Logs prediction differences
- Zero impact on latency

**Example:**
```go
config := ml.ABTestConfig{
    Name:     "model-comparison",
    Strategy: ml.ABTestStrategyHash,
    Variants: []ml.ModelVariant{
        {Name: "control", ModelVersion: "v1.0", Weight: 80.0},
        {Name: "treatment", ModelVersion: "v2.0", Weight: 20.0},
    },
    EnableShadowMode: true,
}
```

### Online Learning Operator

Update models incrementally from streaming data.

**Features:**
- Partial fit on mini-batches
- Automatic checkpointing
- Validation on holdout data
- Learning rate scheduling

**Example:**
```go
config := ml.OnlineLearningConfig{
    ModelName:         "adaptive-model",
    UpdateBatchSize:   32,
    UpdateInterval:    5 * time.Second,
    CheckpointInterval: 5 * time.Minute,
    TargetField:       "label",
    EnableValidation:  true,
}
```

## Configuration

### Full Configuration Example

```yaml
ml:
  enabled: true

  model_registry:
    type: local  # local, s3, gcs
    base_path: ./models

    # S3 configuration
    s3_bucket: my-models-bucket
    s3_region: us-east-1

  models:
    - name: fraud-detector
      version: v2.0
      type: onnx
      path: ./models/fraud-v2.onnx
      enabled: true
      warmup_on_load: true
      tags:
        environment: production
        task: classification

    - name: recommender
      version: v1.5
      type: tensorflow
      path: ./models/recommender-v1.5.tflite
      enabled: true

  feature_store:
    enabled: true
    backend: redis
    ttl: 5m
    redis_addr: localhost:6379

  inference:
    default_batch_size: 100
    default_batch_timeout: 50ms
    max_concurrency: 10
    max_latency: 100ms
    enable_gpu: false

  online_learning:
    enabled: true
    default_update_batch_size: 32
    default_update_interval: 5s
    default_checkpoint_interval: 5m
    checkpoint_dir: ./ml-checkpoints
    default_learning_rate: 0.001
    enable_validation: true
    validation_split: 0.2

  ab_testing:
    enabled: true
    experiments:
      - name: fraud-model-test
        strategy: hash
        split_key: user_id
        enable_shadow_mode: true
        variants:
          - name: control
            model_name: fraud-detector
            model_version: v1.0
            weight: 70.0
            enabled: true
          - name: treatment
            model_name: fraud-detector
            model_version: v2.0
            weight: 30.0
            enabled: true
```

## Examples

### Fraud Detection Pipeline

See [`examples/ml-fraud-detection.go`](../examples/ml-fraud-detection.go) for a complete example:

```bash
go run examples/ml-fraud-detection.go
```

**Features:**
- Real-time transaction scoring
- Feature extraction (amount, category, location)
- Batch inference optimization
- High-risk transaction alerting

### A/B Testing Example

See [`examples/ml-abtest-example.go`](../examples/ml-abtest-example.go):

```bash
go run examples/ml-abtest-example.go
```

**Features:**
- 80/20 traffic split
- Shadow mode comparison
- Consistent user assignment
- Metrics collection

## Performance

### Latency Optimization

**Batching:**
- Batch size: 10-1000 (model dependent)
- Timeout: 10-100ms
- Achieves 10-100x throughput improvement

**Concurrency:**
- Parallel inference execution
- Configurable worker pools
- Non-blocking batch accumulation

**GPU Acceleration:**
```yaml
inference:
  enable_gpu: true
  gpu_device_ids: [0, 1]  # Use GPUs 0 and 1
```

### Benchmarks

Typical performance on commodity hardware:

| Model Type | Batch Size | Latency (P50) | Latency (P99) | Throughput |
|------------|-----------|---------------|---------------|------------|
| ONNX (CPU) | 1 | 5ms | 15ms | 200 req/s |
| ONNX (CPU) | 100 | 50ms | 100ms | 20,000 req/s |
| TFLite (CPU) | 1 | 3ms | 10ms | 300 req/s |
| TFLite (CPU) | 100 | 30ms | 75ms | 30,000 req/s |
| ONNX (GPU) | 100 | 10ms | 25ms | 100,000 req/s |

## Monitoring

### Prometheus Metrics

**Inference Metrics:**
```
gress_ml_inference_latency_seconds{model, version}
gress_ml_predictions_total{model, version, status}
gress_ml_batch_size{model, version}
gress_ml_queue_depth
gress_ml_active_predictions
```

**Feature Extraction:**
```
gress_ml_feature_extraction_time_seconds{feature_set}
gress_ml_features_extracted_total{feature_set, feature_type}
```

**Online Learning:**
```
gress_ml_online_updates_processed_total{model, version}
gress_ml_model_score{model, version, metric}
gress_ml_checkpoint_duration_seconds{model, version}
```

**A/B Testing:**
```
gress_ml_abtest_variant_selections_total{experiment, variant}
gress_ml_abtest_variant_latency_seconds{experiment, variant}
gress_ml_abtest_shadow_comparisons_total{experiment, result}
```

### Grafana Dashboard

Import the included dashboard:
```bash
kubectl apply -f deploy/grafana/ml-dashboard.json
```

**Panels:**
- Inference latency (P50, P95, P99)
- Throughput by model
- Batch size distribution
- Error rates
- A/B test variant distribution
- Model drift scores

### Alerts

**Critical:**
```yaml
- alert: MLInferenceHighLatency
  expr: histogram_quantile(0.99, gress_ml_inference_latency_seconds) > 1.0
  for: 5m

- alert: MLInferenceHighErrorRate
  expr: rate(gress_ml_inference_errors_total[5m]) > 0.05
  for: 5m
```

**Warning:**
```yaml
- alert: MLModelDrift
  expr: gress_ml_model_drift_score > 0.8
  for: 15m
```

## Best Practices

### Model Management

1. **Versioning**: Use semantic versioning (v1.0.0)
2. **Testing**: Validate models before production
3. **Rollback**: Keep previous versions available
4. **Monitoring**: Track model performance metrics

### Performance

1. **Batching**: Tune batch size for your latency requirements
2. **Caching**: Enable feature caching for repeated computations
3. **Resource Limits**: Set appropriate CPU/memory limits
4. **Warmup**: Pre-warm models on startup

### Reliability

1. **Checkpointing**: Regular model checkpoints for online learning
2. **Fallbacks**: Define default predictions for errors
3. **Circuit Breakers**: Protect downstream systems
4. **Health Checks**: Monitor model availability

## Troubleshooting

### Common Issues

**High Latency:**
- Increase batch size
- Enable GPU acceleration
- Reduce model complexity
- Check network/disk I/O

**Memory Issues:**
- Reduce batch size
- Limit concurrent requests
- Use quantized models
- Enable model sharing

**Accuracy Degradation:**
- Check for model drift
- Validate input features
- Review A/B test results
- Retrain with recent data

## License

Apache License 2.0 - See [LICENSE](../LICENSE) for details.
